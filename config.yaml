model:
  n_layer: 4
  n_head: 4
  n_embd: 256
  seq_len: 128
  dropout: 0.0
training:
  batch_size: 16
  micro_batch: 4
  lr: 0.0003
  weight_decay: 0.01
  max_steps: 2000
  warmup_steps: 5
  eval_interval: 20
  save_dir: checkpoints
  seed: 42
data:
  train_path: data/train.jsonl
  val_path: data/val.jsonl
  format: instruct
tokenizer:
  type: hf_tokenizers
  path: tokenizer/tokenizer.json